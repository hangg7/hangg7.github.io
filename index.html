<!doctype html>
<html>
    <head>
        <!-- Global Site Tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-128167050-1">
        </script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-128167050-1');
        </script>
        <script src="js/scramble.js"></script>
        <script src="js/hidebib.js"></script>

        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="chrome=1">
        <title>Hang Gao</title>

        <link rel="stylesheet" href="css/styles.css">
        <link rel="stylesheet" href="css/pygment_trac.css">
        <link rel="stylesheet" href="css/bootstrap.min.css">
        <link rel="stylesheet" href="css/font-awesome.min.css">
        <link rel="stylesheet" href="css/academicons.min.css">
        <link rel="stylesheet" href="css/special.css">
        <meta name="viewport" content="width=device-width">
    </head>

    <body>
        <div class="wrapper">
            <header>
                <br>
                <div align="left">
                    <span class="image avatar" >
                        <img src="assets/avatar.jpg" ></img>
                    </span>
                </div>
                <h1><b>Hang Gao</b></h1>

                <p>
                <a href="https://github.com/hangg7">
                    <font color="#222">
                        <i class="fa fa-github fa-2x" class="icon"></i>
                    </font>
                </a> &nbsp &nbsp
                <a href="https://scholar.google.com/citations?user=IAxkUQkAAAAJ" class="icon">
                    <font color="#222">
                        <i class="ai ai-google-scholar ai-2x" class="icon"></i>
                    </font>
                </a> &nbsp &nbsp
                </p>
                <br>

                <h4>Contact:</h4>
                <p>
                <font face="courier" id="email" style="display:inline;">
                    er@clbuensdeg.eegeh.aky
                    <a href="#" onclick="emailScramble.initAnimatedBubbleSort();return false;">
                        unscramble
                    </a>
                </font>
                <script>
                    emailScramble = new scrambledString(document.getElementById('email'),
                        'emailScramble', 'er@clbuensdeg.eegeh.aky',
                        [7,13,5,8,16,11,22,15,2,9,21,6,4,10,20,12,3,17,0,19,1,14,18]);
                </script>
                </p>
            </header>

            <section class="outer">
                <section  id="about">
                    <h2>About</h2>
                    <p>
                    I am a first year CS Ph.D. student at UC Berkeley.
                    Previously, I obtained my master degree from Columbia where
                    I worked with
                    <a href="http://www.ee.columbia.edu/~sfchang/" style="white-space: nowrap">
                        Shih-Fu Chang</a>. During my undergrad years in
                    Jiao Tong University, I collaborated closely with
                    <a href="http://http://www.cs.cmu.edu/~anind/" style="white-space: nowrap">
                        Anind Dey</a>.
                    </p>
                    <p>
                    My research tries to study pixels from a holistic view for
                    efficient and robust visual representations. I am
                    interested in modeling persistency across three themes:
                    motion, geometry and semantics.
                    </p>
                </section>
                <section id='publications'>
                    <h2>Publications</h2>

                    <b>
                        <a href="./#" class="publications title">
                            Human Motion Prediction with Scene Context
                        </a>
                    </b>
                    <br>
                    Zhe Cao,
                    <u>Hang Gao</u>,
                    Karttikeya Mangalam,
                    Qi-Zhi Cai,
                    Minh Vo,
                    Jitendra Malik
                    <br>
                    <div class="paper" id="cao2020human">
                        Preprint
                        / <a href="javascript:toggleblock('cao2020human_abs')">abstract</a>
                        <p align="justify">
                        <dir id="cao2020human_abs" class="abstract" style="display: none;">
                        Humans have a remarkable ability to make accurate
                        long-term planning conditioned on the surrounding world
                        and their motion history. When facing the same
                        situation, people behave differently according to
                        his/her internal states. In this work, we introduce a
                        computational framework to predict multi-modal human
                        motion while considering scene context. Given a short
                        sequence of 2D human poses history (say 1.5 seconds)
                        and an instantaneous RGB image as inputs, our method is
                        able to predict multiple possible future destinations,
                        perform a long-term (3.5 seconds) 3D path planning
                        towards the destination, and finally generate global 3D
                        human pose sequences respecting the physical
                        constraints of the environment. The key to our approach
                        is to explicitly capture global contextual dependencies
                        between the 2D pose history and the scene by predicting
                        multi-modal 2D goal destinations following by a 3D path
                        to achieve such a goal. We train our model on both
                        real-world data with noisy ground-truth and our
                        newly-created large-scale synthetic data with diverse
                        scenes, characters, and motions. Both quantitative
                        comparisons and qualitative results demonstrate that
                        our method can generate plausible scene-adaptive
                        multi-modal predictions that are not possible with
                        existing state of the arts.
                        </dir>
                        </p>
                        <pre xml:space="preserve" style="display: none;">
<!-- @inproceedings{cao2020human, -->
    <!-- title={Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation}, -->
    <!-- author={Gao, Hang and Zhu, Xizhou and Lin, Steve and Dai, Jifeng}, -->
    <!-- booktitle={International Conference on Learning Representations}, -->
    <!-- year={2020}, -->
<!-- } -->
                        </pre>
                    </div>

                    <b>
                        <a href="https://arxiv.org/pdf/1910.02940.pdf" class="publications title">
                            Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation
                        </a>
                    </b>
                    <br>
                    <u>Hang Gao</u><sup>*</sup>,
                    Xizhou Zhu<sup>*</sup>,
                    Steve Lin,
                    Jifeng Dai
                    <br>
                    <div class="paper" id="gao2020deformable">
                        ICLR 2020
                        / <a href="https://arxiv.org/abs/1910.02940">arxiv</a>
                        / <a href="javascript:toggleblock('gao2020deformable_abs')">abstract</a>
                        / <a href="javascript:togglebib('gao2020deformable')">bib</a>
                        / <a href="https://people.eecs.berkeley.edu/~hangg/deformable-kernels/">website</a>
                        / <a href="https://github.com/hangg7/deformable-kernels/">code</a>
                        <p align="justify">
                        <dir id="gao2020deformable_abs" class="abstract" style="display: none;">
                            Convolutional networks are not aware of an object's
                            geometric variations, which leads to inefficient
                            utilization of model and data capacity. To overcome
                            this issue, recent works on deformation modeling
                            seek to spatially reconfigure the data towards a
                            common arrangement such that semantic recognition
                            suffers less from deformation. This is typically
                            done by augmenting static operators with learned
                            free-form sampling grids in the image space,
                            dynamically tuned to the data and task for adapting
                            the receptive field. Yet adapting the receptive
                            field does not quite reach the actual goal -- what
                            really matters to the network is the
                            <it>effective</it>
                            receptive field (ERF), which reflects how much each
                            pixel contributes. It is thus natural to design
                            other approaches to adapt the ERF directly during
                            runtime.
			    <br>
                            In this work, we instantiate one possible solution
                            as Deformable Kernels (DKs), a family of novel and
                            generic convolutional operators for handling object
                            deformations by directly adapting the ERF while
                            leaving the receptive field untouched. At the heart
                            of our method is the ability to resample the
                            original kernel space towards recovering the
                            deformation of objects. This approach is justified
                            with theoretical insights that the ERF is strictly
                            determined by data sampling locations and kernel
                            values. We implement DKs as generic drop-in
                            replacements of rigid kernels and conduct a series
                            of empirical studies whose results conform with our
                            theories. Over several tasks and standard base
                            models, our approach compares favorably against
                            prior works that adapt during runtime. In addition,
                            further experiments suggest a working mechanism
                            orthogonal and complementary to previous works.
                        </dir>
                        </p>
                        <pre xml:space="preserve" style="display: none;">
@inproceedings{gao2020deformable,
    title={Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation},
    author={Gao, Hang and Zhu, Xizhou and Lin, Steve and Dai, Jifeng},
    booktitle={International Conference on Learning Representations},
    year={2020},
}
                        </pre>
                    </div>

                    <b>
                        <a href="http://arxiv.org/pdf/1812.01233.pdf" class="publications title">
                            Spatio-Temporal Action Graph Networks
                        </a>
                    </b>
                    <br>
                    Roei Herzig<sup>*</sup>,
                    Elad Levi<sup>*</sup>,
                    Huijuan Xu<sup>*</sup>,
                    <u>Hang Gao</u>,
                    Eli Brosh,
                    Xiaolong Wang,
                    Amir Globerson,
                    Trevor Darrell
                    <br>
                    <div class="paper" id="herzig2019spatio">
                        ICCV 2019 Workshop
                        / <a href="https://arxiv.org/abs/1812.01233">arxiv</a>
                        <p align="justify">
			</p>
                    </div>

                    <b>
                        <a href="https://arxiv.org/pdf/1812.00452.pdf" class="publications title">
                            Disentangling Propagation and Generation for Video Prediction
                        </a>
                    </b>
                    <br>
                    <u>Hang Gao</u><sup>*</sup>,
                    Huazhe Xu<sup>*</sup>,
                    Qi-Zhi Cai,
                    Ruth Wang,
                    Fisher Yu,
                    Trevor Darrell
                    <br>
                    <div class="paper" id="gao2019disentangling">
                        ICCV 2019
                        / <a href="https://arxiv.org/abs/1812.00452">arxiv</a>
                        / <a href="javascript:toggleblock('gao2019disentangling_abs')">abstract</a>
                        / <a href="javascript:togglebib('gao2019disentangling')">bib</a>
                        <p align="justify">
                        <dir id="gao2019disentangling_abs" class="abstract" style="display: none;">
                            A dynamic scene has two types of elements: those
                            that move fluidly and can be predicted from
                            previous frames, and those which are disoccluded
                            (exposed) and cannot be extrapolated. Prior
                            approaches to video prediction typically learn
                            either to warp or to hallucinate future pixels, but
                            not both. In this paper, we describe a
                            computational model for high-fidelity video
                            prediction which disentangles motion-specific
                            propagation from motion-agnostic generation. We
                            introduce a confidence-aware warping operator which
                            gates the output of pixel predictions from a flow
                            predictor for non-occluded regions and from a
                            context encoder for occluded regions. Moreover, in
                            contrast to prior works where confidence is jointly
                            learned with flow and appearance using a single
                            network, we compute confidence after a warping
                            step, and employ a separate network to inpaint
                            exposed regions. Empirical results on both
                            synthetic and real datasets show that our
                            disentangling approach provides better occlusion
                            maps and produces both sharper and more realistic
                            predictions compared to strong baselines.
                        </dir>
                        </p>
                        <pre xml:space="preserve" style="display: none;">
@inproceedings{gao2019disentangling,
    title={Disentangling Propagation and Generation for Video Prediction},
    author={Gao, Hang and Xu, Huazhe and Cai, Qi-Zhi
    and Wang, Ruth and Yu, Fisher and Darrell, Trevor},
    booktitle={Proceedings of the IEEE International Conference on Computer Vision},
    year={2019},
}
                        </pre>
                    </div>

                    <b>
                        <a href="https://arxiv.org/pdf/1810.11730.pdf" class="publications title">
                            Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks
                        </a>
                    </b>
                    <br>
                    <u>Hang Gao</u>, Zheng Shou, Alireza Zareian, Hanwang Zhang, Shih-Fu Chang <br>
                    <div class="paper" id="gao2018low">
                        NeurIPS 2018
                        / <a href="https://arxiv.org/abs/1810.11730">arxiv</a>
                        / <a href="javascript:toggleblock('gao2018low_abs')">abstract</a>
                        / <a href="javascript:togglebib('gao2018low')">bib</a>
                        <p align="justify">
                        <dir id="gao2018low_abs" class="abstract" style="display: none;">
                            Deep neural networks suffer from over-fitting and
                            catastrophic forgetting when trained with small
                            data. One natural remedy for this problem is data
                            augmentation, which has been recently shown to be
                            effective. However, previous works either assume
                            that intra-class variances can always be
                            generalized to new classes, or employ naive
                            generation methods to hallucinate finite examples
                            without modeling their latent distributions. In
                            this work, we propose Covariance-Preserving
                            Adversarial Augmentation Networks to overcome
                            existing limits of low-shot learning. Specifically,
                            a novel Generative Adversarial Network is designed
                            to model the latent distribution of each novel
                            class given its related base counterparts. Since
                            direct estimation on novel classes can be
                            inductively biased, we explicitly preserve
                            covariance information as the ``variability'' of
                            base examples during the generation process.
                            Empirical results show that our model can generate
                            realistic yet diverse examples, leading to
                            substantial improvements on the ImageNet benchmark
                            over the state of the art.
                        </dir>
                        </p>
                        <pre xml:space="preserve" style="display: none;">
@inproceedings{gao2018low,
    title={Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks},
    author={Gao, Hang and Shou, Zheng and Zareian, Alireza
    and Zhang, Hanwang and Chang, Shih-Fu},
    booktitle={Advances in Neural Information Processing Systems (NIPS)},
    year={2018},
}
                        </pre>
                    </div>
                    <b>
                        <a href="https://arxiv.org/pdf/1807.08333.pdf" class="publications title">
                            AutoLoc: Weakly-supervised Temporal Action Localization in Untrimmed Videos
                        </a>
                    </b>
                    <br>
                    Zheng Shou,
                    <u>Hang Gao</u>,
                    Lei Zhang,
                    Kazuyuki Miyazawa,
                    Shih-Fu Chang
                    <br>
                    <div class="paper" id="shou2018autoloc">
                        ECCV 2018
                        / <a href="https://arxiv.org/abs/1807.08333">arxiv</a>
                        / <a href="javascript:toggleblock('shou2018autoloc_abs')">abstract</a>
                        / <a href="javascript:togglebib('shou2018autoloc')">bib</a>
                        / <a href="https://github.com/zhengshou/AutoLoc/">code</a>
                        <p align="justify">
                        <dir id="shou2018autoloc_abs" class="abstract" style="display: none;">
                            Temporal Action Localization (TAL) in untrimmed
                            video is important for many applications. But it is
                            very expensive to annotate the segment-level ground
                            truth (action class and temporal boundary). This
                            raises the interest of addressing TAL with weak
                            supervision, namely only video-level annotations
                            are available during training). However, the
                            state-of-the-art weakly-supervised TAL methods only
                            focus on generating good Class Activation Sequence
                            (CAS) over time but conduct simple thresholding on
                            CAS to localize actions. In this paper, we first
                            develop a novel weakly-supervised TAL framework
                            called AutoLoc to directly predict the temporal
                            boundary of each action instance. We propose a
                            novel Outer-Inner-Contrastive (OIC) loss to
                            automatically discover the needed segment-level
                            supervision for training such a boundary predictor.
                            Our method achieves dramatically improved
                            performance: under the IoU threshold 0.5, our
                            method improves mAP on THUMOS'14 from 13.7% to
                            21.2% and mAP on ActivityNet from 7.4% to 27.3%. It
                            is also very encouraging to see that our
                            weakly-supervised method achieves comparable
                            results with some fully-supervised methods.
                        </dir>
                        </p>
                        <pre xml:space="preserve" style="display: none;">
@inproceedings{shou2018autoloc,
    title={AutoLoc: Weakly-supervised Temporal Action
    Localization in Untrimmed Videos},
    author={Shou, Zheng and Gao, Hang and Zhang, Lei
    and Miyazawa, Kazuyuki and Chang, Shih-Fu},
    booktitle = {ECCV},
    year={2018}
}
                        </pre>
                    </div>
                    <b>
                        <a href="http://www.winlab.rutgers.edu/~yychen/papers/ER%20Early%20Recognition%20of%20Inattentive%20Driving%20Leveraging%20Audio%20Devices%20on%20Smartphones.pdf" class="publications title">
                            ER: Early Recognition of Inattentive Driving Events Leveraging Audio Devices on Smartphones
                        </a>
                    </b>
                    <br>
                    Xiangyu Xu,
                    <u>Hang Gao</u>,
                    Jiadi Yu,
                    Yingying Chen,
                    Yanmin Zhu,
                    Guangtao Xue,
                    Minglu Li
                    <br>
                    <div class="paper" id="xu2017er">
                        INFOCOM 2017
                        / <a href="https://ieeexplore.ieee.org/document/8057022">IEEE</a>
                        / <a href="javascript:toggleblock('xu2017er_abs')">abstract</a>
                        / <a href="javascript:togglebib('xu2017er')">bib</a>
                        <p align="justify">
                        <dir id="xu2017er_abs" class="abstract" style="display: none;">
                            Real-time driving behavior monitoring is a corner
                            stone to improve driving safety. Most of the
                            existing studies on driving behavior monitoring
                            using smartphones only provide detection results
                            after an abnormal driving behavior is finished, not
                            sufficient for driver alert and avoiding car
                            accidents. In this paper, we leverage existing
                            audio devices on smartphones to realize early
                            recognition of inattentive driving events including
                            Fetching Forward, Picking up Drops, Turning Back
                            and Eating or Drinking. Through empirical studies
                            of driving traces collected in real driving
                            environments, we find that each type of inattentive
                            driving event exhibits unique patterns on Doppler
                            profiles of audio signals. This enables us to
                            develop an Early Recognition system, ER, which can
                            recognize inattentive driving events at an early
                            stage and alert drivers timely. ER employs machine
                            learning methods to first generate binary
                            classifiers for every pair of inattentive driving
                            events, and then develops a modified vote mechanism
                            to form a multi-classifier for all inattentive
                            driving events along with other driving behaviors.
                            It next turns the multi-classifier into a gradient
                            model forest to achieve early recognition of
                            inattentive driving. Through extensive experiments
                            with 8 volunteers driving for about half a year, ER
                            can achieve an average total accuracy of 94.80% for
                            inattentive driving recognition and recognize over
                            80% inattentive driving events before the event is
                            50% finished.
                        </dir>
                        </p>
                        <pre xml:space="preserve" style="display: none;">
@inproceedings{xu2017er,
    title={ER: Early recognition of inattentive driving
    leveraging audio devices on smartphones},
    author={Xu, Xiangyu and Gao, Hang and Yu, Jiadi and
    Chen, Yingying and Zhu, Yanmin and Xue, Guangtao and Li, Minglu},
    booktitle={INFOCOM 2017-IEEE Conference on Computer
    Communications, IEEE},
    pages={1--9},
    year={2017},
    organization={IEEE}
}
                        </pre>
                    </div>
                </section>
            </section>
        </div>
        <section id="footer">
            <div class="container">
                <ul class="copyright">
                    <li>&copy; Hang Gao <text id="yr"></text></li>
                    <li>Courtesy:
                        <a href="http://linji.me/">L</a>
                        <a href="http://hxu.rocks/">X</a>
                        <a href="https://people.eecs.berkeley.edu/~pathak/">P</a>
                        <a href="http://people.csail.mit.edu/junyanz/">Z</a>
                    </li>
                </ul>
            </div>
        </section>
    </body>
    <script>
        var yr = document.lastModified;
        document.getElementById("yr").innerHTML = yr.substr(6, 4);
    </script>
    <script src="js/fixscale.js"></script>
</html>
