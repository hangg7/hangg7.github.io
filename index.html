<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Hang Gao</title>
    <meta charset="utf-8" />
    <meta name="author" content="Hang Gao" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" type="text/css" href="stylesheet.css" />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css?family=Lucida+Grande"
    />
    <link
      rel="icon"
      href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒ¿</text></svg>"
    />
  </head>

  <body>
    <table
      style="
        width: 100%;
        max-width: 800px;
        border: 0px;
        border-spacing: 0px;
        border-collapse: separate;
        margin-right: auto;
        margin-left: auto;
      "
    >
      <tbody>
        <tr style="padding: 0px">
          <td style="padding: 0px">
            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <td style="padding: 2.5%; width: 100%; max-width: 37%">
                  <a href="assets/hang.png"
                    ><img
                      style="
                        width: 100%;
                        max-width: 100%;
                        border: 2px solid #000;
                        border-radius: 0%;
                        display: block;
                      "
                      alt="profile photo"
                      src="assets/hang.png"
                      class="hoverZoomLink"
                  /></a>
                </td>
                <td style="padding: 2.5%; width: 63%; vertical-align: middle">
                  <p style="text-align: center">
                    <name>Hang Gao</name>
                  </p>
                  <p>
                    I am a Ph.D. student at
                    <a
                      href="https://www.berkeley.edu/"
                      style="white-space: nowrap"
                      >UC Berkeley</a
                    >, working on computer vision and graphics, advised by
                    <a
                      href="https://people.eecs.berkeley.edu/~kanazawa/"
                      style="white-space: nowrap"
                    >
                      Angjoo Kanazawa</a
                    >.
                  </p>
                  <p>
                    I did my undergrad at
                    <a
                      href="https://en.sjtu.edu.cn/"
                      style="white-space: nowrap"
                      >Jiao Tong University</a
                    >
                    and got my master from
                    <a
                      href="https://www.columbia.edu/"
                      style="white-space: nowrap"
                      >Columbia</a
                    >. I have spent two summers at
                    <a
                      href="https://research.adobe.com/"
                      style="white-space: nowrap"
                      >Adobe Research</a
                    >
                    in 2021 and 2022, working with
                    <a
                      href="https://research.adobe.com/"
                      style="white-space: nowrap"
                      >Bryan Russell</a
                    >.
                  </p>
                  <p style="text-align: right">
                    <a href="mailto:hangg@berkeley.edu"
                      ><i class="fa-solid fa-envelope fa-xl"></i
                    ></a>
                    <a
                      href="https://scholar.google.com/citations?hl=en&user=IAxkUQkAAAAJ"
                      ><i class="ai ai-google-scholar fa-xl"></i
                    ></a>
                    <a href="https://github.com/hangg7/"
                      ><i class="fa-brands fa-github fa-xl"></i
                    ></a>
                  </p>
                </td>
              </tbody>
            </table>
            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td
                    style="padding: 20px; width: 100%; vertical-align: middle"
                  >
                    <heading>Research</heading>
                    <p>
                      I am currently interested in non-rigid 3D reconstruction
                      and neural rendering. Particularly, I am excited about
                      photorealistic capture of dynamics in-the-wild from
                      consumer-level devices.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr onmouseout="dycheck_stop()" onmouseover="dycheck_start()">
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <div class="one" style="border: 2px solid #000">
                      <div class="two" id="dycheck_teaser">
                        <video width="100%" height="100%" muted autoplay loop>
                          <source
                            src="assets/dycheck_after.mp4"
                            type="video/mp4"
                          />
                          Your browser does not support the video tag.
                        </video>
                      </div>
                      <img src="assets/dycheck_before.png" width="160" />
                    </div>
                    <script type="text/javascript">
                      function dycheck_start() {
                        document.getElementById(
                          "dycheck_teaser"
                        ).style.opacity = "1";
                      }
                      function dycheck_stop() {
                        document.getElementById(
                          "dycheck_teaser"
                        ).style.opacity = "0";
                      }
                      dycheck_stop();
                    </script>
                  </td>
                  <td style="padding: 20px; width: 75%; vertical-align: middle">
                    <a href="https://hangg7.com/dycheck">
                      <papertitle
                        >Monocular Dynamic View Synthesis: A Reality
                        Check</papertitle
                      >
                    </a>
                    <br />
                    <strong>Hang Gao</strong>,
                    <a href="https://www.liruilong.cn/">Ruilong Li</a>,
                    <a href="https://shubhtuls.github.io/">Shubham Tulsiani</a>,
                    <a href="https://bryanrussell.org/">Bryan Russell</a>,
                    <a href="https://people.eecs.berkeley.edu/~kanazawa/"
                      >Angjoo Kanazawa</a
                    >
                    <br />
                    <em>NeurIPS</em>, 2022
                    <br />
                    <a href="https://hangg7.com/dycheck/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2210.13445">arXiv</a>
                    /
                    <a href="https://youtu.be/WwESsNivJP8">video</a>
                    /
                    <a href="https://github.com/KAIR-BAIR/dycheck/">code</a>
                    <p></p>
                    <p>
                      We show a discrepancy between the practical captures and
                      the existing experimental protocols in dynamic view
                      synthesis from monocular video.
                    </p>
                  </td>
                </tr>
                <tr onmouseout="hmp_stop()" onmouseover="hmp_start()">
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <div class="one" style="border: 2px solid #000">
                      <div class="two" id="hmp_teaser">
                        <img src="assets/hmp_after.png" width="160" />
                      </div>
                      <img src="assets/hmp_before.png" width="160" />
                    </div>
                    <script type="text/javascript">
                      function hmp_start() {
                        document.getElementById("hmp_teaser").style.opacity =
                          "1";
                      }
                      function hmp_stop() {
                        document.getElementById("hmp_teaser").style.opacity =
                          "0";
                      }
                      hmp_stop();
                    </script>
                  </td>
                  <td style="padding: 20px; width: 75%; vertical-align: middle">
                    <a href="https://zhec.github.io/hmp/">
                      <papertitle
                        >Long-term Human Motion Prediction with Scene
                        Context</papertitle
                      >
                    </a>
                    <br />
                    <a href="https://zhec.github.io/">Zhe Cao</a>,
                    <strong>Hang Gao</strong>,
                    <a href="https://karttikeya.github.io/"
                      >Karttikeya Mangalam</a
                    >,
                    <a
                      href="https://scholar.google.com/citations?user=oyh-YNwAAAAJ&hl=en"
                      >Qi-Zhi Cai</a
                    >, <a href="https://minhpvo.github.io/">Minh Vo</a>,
                    <a href="http://people.eecs.berkeley.edu/~malik/"
                      >Jitendra Malik</a
                    >
                    <br />
                    <em>ECCV</em>, 2020 &nbsp
                    <font color="red"
                      ><strong>(Oral Presentation)</strong></font
                    >
                    <br />
                    <a href="https://zhec.github.io/hmp/">project page</a>
                    /
                    <a href="https://arxiv.org/abs/2007.03672">arXiv</a>
                    /
                    <a href="https://zhec.github.io/hmp/eccv-long.mp4">video</a>
                    /
                    <a href="https://github.com/ZheC/GTA-IM-Dataset">code</a>
                    <p></p>
                    <p>
                      Understanding scene context from an image helps to predict
                      long-term, diverse human motion in 3D.
                    </p>
                  </td>
                </tr>
                <tr onmouseout="dk_stop()" onmouseover="dk_start()">
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <div class="one" style="border: 2px solid #000">
                      <div class="two" id="dk_teaser">
                        <img src="assets/dk_after.png" width="160" />
                      </div>
                      <img src="assets/dk_before.png" width="160" />
                    </div>
                    <script type="text/javascript">
                      function dk_start() {
                        document.getElementById("dk_teaser").style.opacity =
                          "1";
                      }
                      function dk_stop() {
                        document.getElementById("dk_teaser").style.opacity =
                          "0";
                      }
                      dk_stop();
                    </script>
                  </td>
                  <td style="padding: 20px; width: 75%; vertical-align: middle">
                    <a href="https://hangg7.com/deformable-kernels/">
                      <papertitle>
                        Deformable Kernels: Adapting Effective Receptive Fields
                        for Object Deformation
                      </papertitle>
                    </a>
                    <br />
                    <strong>Hang Gao*</strong>,
                    <a
                      href="https://scholar.google.com/citations?user=02RXI00AAAAJ&hl=en"
                      >Xizhou Zhu*</a
                    >,
                    <a
                      href="https://scholar.google.com/citations?user=c3PYmxUAAAAJ&hl=en&oi=ao"
                      >Steve Lin</a
                    >,
                    <a href="https://jifengdai.org/">Jifeng Dai</a>
                    <br />
                    <em>ICLR</em>, 2020
                    <br />
                    <a href="https://hangg7.com/deformable-kernels/"
                      >project page</a
                    >
                    /
                    <a href="https://arxiv.org/abs/1910.02940v2">arXiv</a>
                    /
                    <a href="https://github.com/hangg7/deformable-kernels/"
                      >code</a
                    >
                    <p></p>
                    <p>
                      By learning an instance-adaptive convolutional operator
                      through 2D deformation in kernel space, we can adapt the
                      effective receptive field at runtime.
                    </p>
                  </td>
                </tr>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <div class="one" style="border: 2px solid #000">
                      <img src="assets/stag.png" width="160" />
                    </div>
                  </td>
                  <td style="padding: 20px; width: 75%; vertical-align: middle">
                    <a href="http://arxiv.org/abs/1812.01233">
                      <papertitle
                        >Spatio-Temporal Action Graph Networks</papertitle
                      >
                    </a>
                    <br />
                    <a href="https://roeiherz.github.io/">Roei Herzig*</a>,
                    <a
                      href="https://scholar.google.com/citations?user=TQw8WLEAAAAJ&hl=en&oi=sra"
                      >Elad Levi*</a
                    >,
                    <a href="https://visionlanguagelab.github.io/"
                      >Huijuan Xu*</a
                    >, <strong>Hang Gao</strong>, Eli Brosh,
                    <a href="https://xiaolonw.github.io/">Xiaolong Wang</a>,
                    <a href="https://cs3801.wixsite.com/amirgloberson"
                      >Amir Globerson</a
                    >,
                    <a href="https://people.eecs.berkeley.edu/~trevor/"
                      >Trevor Darrell</a
                    >
                    <br />
                    <em>ICCV Workshop</em>, 2019
                    <br />
                    <a href="http://arxiv.org/abs/1812.01233">arXiv</a>
                    <p></p>
                    <p>
                      We model video as a spatial-temporal relational graph for
                      action recognition and find that the second order affinity
                      (affinity between edges) is surprisingly helpful.
                    </p>
                  </td>
                </tr>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <div class="one" style="border: 2px solid #000">
                      <img src="assets/dpg.png" width="160" />
                    </div>
                  </td>
                  <td style="padding: 20px; width: 75%; vertical-align: middle">
                    <a href="https://arxiv.org/abs/1812.00452">
                      <papertitle>
                        Disentangling Propagation and Generation for Video
                        Prediction
                      </papertitle>
                    </a>
                    <br />
                    <strong>Hang Gao*</strong>,
                    <a href="http://hxu.rocks/">Huazhe Xu</a>,
                    <a
                      href="https://scholar.google.com/citations?user=oyh-YNwAAAAJ&hl=en"
                      >Qi-Zhi Cai</a
                    >,
                    <a
                      href="https://scholar.google.com/citations?user=qrHfaicAAAAJ&hl=en&oi=sra"
                      >Ruth Wang</a
                    >, <a href="https://www.yf.io/">Fisher Yu</a>,
                    <a href="https://people.eecs.berkeley.edu/~trevor/"
                      >Trevor Darrell</a
                    >
                    <br />
                    <em>ICCV</em>, 2019
                    <br />
                    <a href="https://arxiv.org/abs/1812.00452">arXiv</a>
                    <p></p>
                    <p>
                      High fidelity video prediction is easier if we disentangle
                      the flow propagation from frame generation.
                    </p>
                  </td>
                </tr>
                <tr onmouseout="covgan_stop()" onmouseover="covgan_start()">
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <div class="one" style="border: 2px solid #000">
                      <div class="two" id="covgan_teaser">
                        <img src="assets/covgan_after.png" width="160" />
                      </div>
                      <img src="assets/covgan_before.png" width="160" />
                    </div>
                    <script type="text/javascript">
                      function covgan_start() {
                        document.getElementById("covgan_teaser").style.opacity =
                          "1";
                      }
                      function covgan_stop() {
                        document.getElementById("covgan_teaser").style.opacity =
                          "0";
                      }
                      covgan_stop();
                    </script>
                  </td>
                  <td style="padding: 20px; width: 75%; vertical-align: middle">
                    <a href="https://arxiv.org/abs/1810.11730">
                      <papertitle>
                        Low-shot Learning via Covariance-Preserving Adversarial
                        Augmentation Networks
                      </papertitle>
                    </a>
                    <br />
                    <strong>Hang Gao</strong>,
                    <a href="http://www.columbia.edu/~zs2262/">Zheng Shou</a>,
                    <a
                      href="https://scholar.google.com/citations?user=Ioe0SGsAAAAJ&hl=en"
                      >Alireza Zareian</a
                    >,
                    <a href="https://mreallab.github.io/index.html"
                      >Hanwang Zhang</a
                    >,
                    <a href="https://www.ee.columbia.edu/~sfchang/"
                      >Shih-Fu Chang</a
                    >
                    <br />
                    <em>NeurIPS</em>, 2018
                    <br />
                    <a href="https://arxiv.org/abs/1810.11730">arXiv</a>
                    <p></p>
                    <p>
                      We use learned feature augmentation to train low-shot
                      classifiers.
                    </p>
                  </td>
                </tr>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <div class="one" style="border: 2px solid #000">
                      <img src="assets/autoloc.png" width="160" />
                    </div>
                  </td>
                  <td style="padding: 20px; width: 75%; vertical-align: middle">
                    <a href="https://arxiv.org/abs/1807.08333">
                      <papertitle>
                        AutoLoc: Weakly-supervised Temporal Action Localization
                        in Untrimmed Videos
                      </papertitle>
                    </a>
                    <br />
                    <a href="http://www.columbia.edu/~zs2262/">Zheng Shou</a>,
                    <strong>Hang Gao</strong>,
                    <a href="https://www.leizhang.org/">Lei Zhang</a>,
                    <a
                      href="https://scholar.google.com/citations?user=P_GRGwoAAAAJ&hl=en&oi=sra"
                      >Kazuyuki Miyazawa</a
                    >,
                    <a href="https://www.ee.columbia.edu/~sfchang/"
                      >Shih-Fu Chang</a
                    >
                    <br />
                    <em>ECCV</em>, 2018
                    <br />
                    <a href="https://arxiv.org/abs/1807.08333">arXiv</a>
                    /
                    <a href="https://github.com/zhengshou/AutoLoc/">code</a>
                    <p></p>
                    <p>
                      We propose a weakly-supervised method for temporal action
                      localization by maximizing the difference inside and
                      outside the localization box.
                    </p>
                  </td>
                </tr>
                <tr>
                  <td style="padding: 20px; width: 25%; vertical-align: middle">
                    <div class="one" style="border: 2px solid #000">
                      <img src="assets/er.png" width="160" />
                    </div>
                  </td>
                  <td style="padding: 20px; width: 75%; vertical-align: middle">
                    <a href="https://ieeexplore.ieee.org/document/8057022">
                      <papertitle>
                        ER: Early Recognition of Inattentive Driving Events
                        Leveraging Audio Devices on Smartphones
                      </papertitle>
                    </a>
                    <br />
                    <a href="https://shawnnxu.github.io/">Xiangyu Xu</a>,
                    <strong>Hang Gao</strong>,
                    <a href="https://www.cs.sjtu.edu.cn/~jdyu/">Jiadi Yu</a>,
                    <a
                      href="https://scholar.google.com/citations?user=jCZaWOEAAAAJ&hl=en&oi=sra"
                      >Yingying Chen</a
                    >,
                    <a href="https://www.cs.sjtu.edu.cn/~yzhu/">Yanmin Zhu</a>,
                    <a href="https://www.cs.sjtu.edu.cn/~xue-gt/"
                      >Guangtao Xue</a
                    >,
                    <a
                      href="https://scholar.google.com/citations?user=cFW1n8YAAAAJ&hl=en"
                      >Minglu Li</a
                    >
                    <br />
                    <em>INFOCOM</em>, 2017
                    <br />
                    <a href="https://ieeexplore.ieee.org/document/8057022"
                      >IEEE</a
                    >
                    <p></p>
                    <p>
                      We developed a audio-based early recognition system for
                      inattentive driving events through Doppler effect.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
            <table
              width="100%"
              align="center"
              border="0"
              cellspacing="0"
              cellpadding="20"
            >
              <tr>
                <td>
                  <br />
                  <p align="right">
                    <font size="2">
                      Yet another
                      <a href="https://jonbarron.info">Jon Barron</a> website
                      (with minor tweaks).
                      <br />
                      Last updated October 2022.
                    </font>
                  </p>
                </td>
              </tr>
            </table>
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
